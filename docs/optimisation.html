<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Euan Enticott" />


<title>Optimisation</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="site_libs/anchor-sections-1.0/anchor-sections.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Portfolio 2</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="debugging.html">Profiling and Debugging</a>
</li>
<li>
  <a href="matrices.html">Matrices</a>
</li>
<li>
  <a href="optimisation.html">Optimisation</a>
</li>
<li>
  <a href="Integration.html">Integration</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Optimisation</h1>
<h4 class="author">Euan Enticott</h4>
<h4 class="date">15/12/2020</h4>

</div>


<div id="r-markdown" class="section level2">
<h2>R Markdown</h2>
<p>In general optimisation is about obtaining the best solution from all feasible solutions. There are two types, continuous and discrete. For continuous optimisation we try to minimise a function (if you want to maximise you can minimise the negative). The standard form of an optimisation problem is:</p>
<p><span class="math display">\[\begin{split} \textrm{arg}\min_{x}f(x) \textrm{ subject to } &amp; g_{i}(x) \leq 0 \textrm{ for } i = 1,...,m \\  &amp; h_{j}(x) = 0 \textrm{ for } j =1,...,p \end{split}\]</span></p>
<p>where <span class="math inline">\(f:\mathbb{R}^{N} \rightarrow \mathbb{R}\)</span> is the objective function and <span class="math inline">\(g_{i}\)</span> and <span class="math inline">\(h_{j}\)</span> are constraints. The most common <code>R</code> function for constrained optimisation is <code>constrOptim</code>.</p>
<div id="one-dimensional-optimisation" class="section level3">
<h3>One-Dimensional Optimisation</h3>
<p>We examine the function:</p>
<pre class="r"><code>f = function(x) 2*sin(3*x) + 0.4*x^2 - 2
curve(f, from = -3, to = 3)</code></pre>
<p><img src="optimisation_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="the-optimize-function" class="section level3">
<h3>The <code>optimize</code> function</h3>
<p>This can be used on functions of one dimension. We call:</p>
<pre class="r"><code>optimize(f, interval = c(-3, 3))</code></pre>
<pre><code>## $minimum
## [1] -0.501298
## 
## $objective
## [1] -3.895006</code></pre>
<pre class="r"><code>optimize(f, interval = c(-0.8, 2))</code></pre>
<pre><code>## $minimum
## [1] 1.503514
## 
## $objective
## [1] -3.055174</code></pre>
<p>We see that <code>optimize</code> does indeed find the global minimum here, but changing the interval, even when the global minimum is still within can change the result. The function is based on the golden section search which can work well for a unimodal function where the minimum is within the interval. This is rarely the case but its one advantage is that it requires no evaluation of derivatives.</p>
</div>
<div id="newtons-method" class="section level3">
<h3>Newton’s Method</h3>
<p>This aims to find the root of a function <span class="math inline">\(g\)</span>, it relies on the Taylor expansion at <span class="math inline">\(x_{0}\)</span>. We iterively apply: <span class="math display">\[x \leftarrow x - \frac{g(x)}{g&#39;(x)} \]</span> To apply to an optimisation problem we solve for <span class="math inline">\(f&#39;(x)\)</span> = 0. If the initial guess is close to a local minimum then the convergence is rapid but if it is far away it can converge to any local minimum or maximum. It also requires second derivatives which are not always available.</p>
</div>
<div id="multi-dimensional-optimisation" class="section level3">
<h3>Multi-dimensional Optimisation</h3>
<p>There are three types of multi-dimensional algorithms:</p>
<ul>
<li><p>Simplex methods - only use value of the function</p></li>
<li><p>Gradient type methods - use value of function and gradient vector</p></li>
<li><p>Newton type methods - Use value of function, gradient vector and Hessian matrix (or approximation)</p></li>
</ul>
<p>In <code>R</code> these optimisations can be done through the <code>nlm</code> function which uses a Newton-type or <code>optim</code> which has a choice.</p>
<div id="simplex-methods" class="section level4">
<h4>Simplex Methods</h4>
<p>The Nelder-Mead is the standard simplex method. It is a directed search comparing values at various points. It has the advantage of not requiring derivatives but it only converges to local minimima, it can converge to non-stationary points and it is slow.</p>
</div>
<div id="gradient-type-methods" class="section level4">
<h4>Gradient type methods</h4>
<p>The most popular method here is gradient descent. In order to minimise a differentiable function <span class="math inline">\(f(x)\)</span> we iterively take: <span class="math display">\[x \leftarrow x - \gamma\nabla f(x) \]</span> where <span class="math inline">\(\gamma\)</span> is a small step value. This is simple but is slow can can zigzag on certain functions. <code>optim</code> uses the conjugate gradient (CG) which is an improved version. The main idea is that each step should be conjugate toward search directions at previous step to avoid zigzag. If the function is quadratic then CG is guaranteed to reach the minimum in n-steps where function is n-dimensional. It performs better than steepest descent in all cases but not as well as Newton type methods.</p>
</div>
<div id="newton-type-methods" class="section level4">
<h4>Newton type methods</h4>
<p>In the multi dimensional case Newton’s method becomes:</p>
<p><span class="math display">\[x \leftarrow [H f(x)]^{-1} \nabla f(x) \]</span></p>
<p>where we need the inverse of the Hessian. This can be expensive for multivariate problems as the Hessian can be very large. Quasi-Newton methods aim to replace this computation by a reasonable estimate. The most common quasi-newton method is <code>BFGS</code> which can be implemented by <code>optim</code>. The Hessian is approximated by <span class="math inline">\(B_{k}\)</span> and it’s inverse <span class="math inline">\(B_{k+1}^{-1}\)</span> is computed using <span class="math inline">\(B_{k}^{-1}\)</span>. As <span class="math inline">\(B_{k}\)</span> are dense this can cause memory problems in high dimensions, <code>L-BFGS</code> is the low memory version which stores a few vectors rather than a full approximation.</p>
</div>
<div id="simulated-annealing" class="section level4">
<h4>Simulated annealing</h4>
<p>Simulated annealing claims to always find the global minimum, but it is slow and can often fail to find the global minimum regardless. It only uses the value of the function like the simplex method. In <code>optim</code> the implementation is done with a Metropolis function for acceptance probability.</p>
</div>
</div>
<div id="nonlinear-least-squares-problems" class="section level3">
<h3>Nonlinear least squares problems</h3>
<p>With dataset <span class="math inline">\((x_{1}, y_{1}), ...,(x_{m}, y_{m})\)</span> and model <span class="math inline">\(y = f(x, \beta)\)</span> where <span class="math inline">\(\beta\)</span> are the parameters of the model <span class="math inline">\(f\)</span> we define residuals <span class="math inline">\(r_{i} (\beta) = y_{i} - f(x_{i}, \beta)\)</span>. In a nonlinear least squares problem we aim to find <span class="math inline">\(\beta\)</span> such that the sum of squared residuals is minimised. There a subclass of optimisation algorithms designed for nonlinear functions.</p>
<p>The Gauss-Newton algorithm is related to Newton’s method. The difference is that Gauss-Newton approximates the Hesian matrix of the objective functoin using Jacobian matrix <span class="math inline">\(J_{r}\)</span> of the residue function. It approximates the Hesian with <span class="math inline">\(J_{r}^{t}J_{r}\)</span> and then iterively does: <span class="math display">\[\beta \leftarrow \beta - (J_{r}^{t}J_{r})^{-1}J_{r}^{t}r(\beta)\]</span> This works well in practice but is not guaranteed to converge. The Levenberg-Marquadt algorithm addresses this using damping, instead pretending the Hessian is <span class="math inline">\(J_{r}^{t}J_{r} + \lambda I\)</span>. This is guaranteed to be invertible for some <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div id="stochastic-gradient-descent" class="section level3">
<h3>Stochastic gradient descent</h3>
<p>So far the algorithms have all computed gradient vectors using all of the dataset. In big data applications, the dataset can be too large to hold in memory. This motivates Stochastic Gradient Descent (SGD). This takes the gradient direction using only a few data samples. It is popular for training neural networks where full back propogation is computationally expensive. The simplest form is: <span class="math display">\[\beta \leftarrow \beta - \gamma r_{i} \nabla r_{i}(\beta)\]</span> where <span class="math inline">\(i\)</span> cycles through all data points. It can also be done by combining data samples into ‘minibatches’ and updating <span class="math inline">\(\beta\)</span> with the samples in each minibatch.</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
