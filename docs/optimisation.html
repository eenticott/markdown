<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Optimisation</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="site_libs/anchor-sections-1.0/anchor-sections.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Portfolio 2</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="debugging.html">Profiling and Debugging</a>
</li>
<li>
  <a href="matrices.html">Matrices</a>
</li>
<li>
  <a href="optimisation.html">Optimisation</a>
</li>
<li>
  <a href="Integration.html">Integration</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Optimisation</h1>

</div>


<div id="r-markdown" class="section level2">
<h2>R Markdown</h2>
<p>In general optimisation is about obtaining the best solution from all feasible solutions. There are two types, continuous and discrete. For continuous optimisation we try to minimise a function (if you want to maximise you can minimise the negative). The standard form of an optimisation problem is:</p>
<p><span class="math display">\[\begin{split} \textrm{arg}\min_{x}f(x) \textrm{ subject to } &amp; g_{i}(x) \leq 0 \textrm{ for } i = 1,...,m \\  &amp; h_{j}(x) = 0 \textrm{ for } j =1,...,p \end{split}\]</span></p>
<p>where <span class="math inline">\(f:\mathbb{R}^{N} \rightarrow \mathbb{R}\)</span> is the objective function and <span class="math inline">\(g_{i}\)</span> and <span class="math inline">\(h_{j}\)</span> are constraints. The most common <code>R</code> function for constrained optimisation is <code>constrOptim</code>.</p>
<div id="one-dimensional-optimisation" class="section level3">
<h3>One-Dimensional Optimisation</h3>
<p>We examine the function:</p>
<pre class="r"><code>f = function(x) 2*sin(3*x) + 0.4*x^2 - 2
curve(f, from = -3, to = 3)</code></pre>
<p><img src="optimisation_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="the-optimize-function" class="section level3">
<h3>The <code>optimize</code> function</h3>
<p>This can be used on functions of one dimension. We call:</p>
<pre class="r"><code>optimize(f, interval = c(-3, 3))</code></pre>
<pre><code>## $minimum
## [1] -0.501298
## 
## $objective
## [1] -3.895006</code></pre>
<pre class="r"><code>optimize(f, interval = c(-0.8, 2))</code></pre>
<pre><code>## $minimum
## [1] 1.503514
## 
## $objective
## [1] -3.055174</code></pre>
<p>We see that <code>optimize</code> does indeed find the global minimum here, but changing the interval, even when the global minimum is still within, can change the result. The function is based on the golden section search which <em>can</em> work well for a unimodal function where the minimum is within the interval. This is rarely the case but its one advantage is that it requires no evaluation of derivatives and so can be widely used.</p>
</div>
<div id="newtons-method" class="section level3">
<h3>Newton’s Method</h3>
<p>This aims to find the root of a function <span class="math inline">\(g\)</span>, it relies on the Taylor expansion at <span class="math inline">\(x_{0}\)</span>. We iteratively apply: <span class="math display">\[x \leftarrow x - \frac{g(x)}{g&#39;(x)} \]</span> To apply to an optimisation problem, we solve for <span class="math inline">\(f&#39;(x)\)</span> = 0. If the initial guess is close to a local minimum then the convergence is rapid but if it is far away it can converge to any local minimum or maximum. It also requires second derivatives which are not always available. We apply this to the same function:</p>
<pre class="r"><code>f_sym = expression(2*sin(3*x) + 0.4*x^2 - 2)
f1_sym = D(f_sym, &#39;x&#39;); f1 = function(x) eval(f1_sym)
f2_sym = D(f1_sym, &#39;x&#39;); f2 = function(x) eval(f2_sym)

newton &lt;- function(f1, f2, init, tol, print) {
  x = init
  change = tol + 1
  while (change &gt; tol) {
    x_pre = x
    x = x - f1(x) / f2(x); if(print) {cat(x, &#39;\n&#39;)}
    change = abs(x - x_pre)
  }
  return(x)
}

newton(f1_sym, f2_sym, 1, 1e-6, TRUE)</code></pre>
<pre><code>## -1.953725 
## -1.358745 
## -1.698802 
## -1.642861 
## -1.644482 
## -1.644483</code></pre>
<pre><code>## [1] -1.644483</code></pre>
<p>We can see that it has converged to a local maximum instead of a minimum. We try with different starting values:</p>
<pre class="r"><code>for (i in seq(-3, 3, 0.5)) {
  print(newton(f1_sym, f2_sym, i, 1e-6, FALSE))
}</code></pre>
<pre><code>## [1] 3.503192
## [1] -2.504503
## [1] 1.503517
## [1] -1.644483
## [1] -6.074247
## [1] -0.5013021
## [1] -6.460755
## [1] 0.5479749
## [1] -1.644483
## [1] 1.503517
## [1] 0.5479749
## [1] 2.742791
## [1] 2.742791</code></pre>
<p>So we see that in this case Newton’s method only finds the correct minimum two times out of those tried.</p>
</div>
<div id="multi-dimensional-optimisation" class="section level3">
<h3>Multi-dimensional Optimisation</h3>
<p>There are three types of multi-dimensional algorithms:</p>
<ul>
<li><p>Simplex methods - only use value of the function</p></li>
<li><p>Gradient type methods - use value of function and gradient vector</p></li>
<li><p>Newton type methods - use value of function, gradient vector and Hessian matrix (or approximation)</p></li>
</ul>
<p>In <code>R</code> these optimisations can be done through the <code>nlm</code> function which uses a Newton-type or <code>optim</code> which has a choice.</p>
<p>We will attempt to minimise the problem:</p>
<pre class="r"><code>f = function(x, y) {
  return(cos(x*y) *sin(2*x*y-5) + 2 * cos(3*x + y - 5) + 2*(x)^2 + 2*(y)^2 + 2)
}

library(pracma) # for meshgrid
x = seq(-2, 2, length = 500)
y = seq(-2, 2, length = 500)
gridXY = meshgrid(x, y)
z = matrix(mapply(f, gridXY$X, gridXY$Y), nrow = 500)
contour(x, y, z)</code></pre>
<p><img src="optimisation_files/figure-html/2dfunc-1.png" width="672" /></p>
<pre class="r"><code>min(z)</code></pre>
<pre><code>## [1] 1.560611</code></pre>
<p>We need to get a function for the derivatives and Hessian.</p>
<pre class="r"><code>f1 = deriv(expression(cos(x*y) *sin(2*x*y-5) + 2 * cos(3*x + y - 5) + 2*(x)^2 + 2*(y)^2 + 2), namevec = c(&#39;x&#39;, &#39;y&#39;), function.arg=T, hessian=T)
f1(0,0)</code></pre>
<pre><code>## [1] 3.526249
## attr(,&quot;gradient&quot;)
##              x         y
## [1,] -5.753546 -1.917849
## attr(,&quot;hessian&quot;)
## , , x
## 
##              x         y
## [1,] -1.105919 -1.134649
## 
## , , y
## 
##              x        y
## [1,] -1.134649 3.432676</code></pre>
<p>We try using Newton’s method through <code>nlm</code> to minimise the function:</p>
<pre class="r"><code>optim_results &lt;- data.frame(minimum = numeric(), times = numeric())

start = Sys.time()
nlm_grad &lt;- nlm(function(x) f1(x[1], x[2]), c(1,1))
end = Sys.time()
nlm_grad</code></pre>
<pre><code>## $minimum
## [1] 3.786106
## 
## $estimate
## [1] -1.0618650 -0.6920803
## 
## $gradient
## [1] -7.091298e-07 -5.404082e-07
## 
## $code
## [1] 1
## 
## $iterations
## [1] 9</code></pre>
<pre class="r"><code>optim_results[&quot;nlm_with_grad&quot;,] &lt;- c(nlm_grad$value,end - start)

start = Sys.time()
nlm_nograd &lt;- nlm(function(x) f(x[1], x[2]), c(1,1))
end = Sys.time()
nlm_nograd</code></pre>
<pre><code>## $minimum
## [1] 3.786106
## 
## $estimate
## [1] -1.0618654 -0.6920804
## 
## $gradient
## [1] -4.868035e-07 -3.863576e-07
## 
## $code
## [1] 1
## 
## $iterations
## [1] 10</code></pre>
<pre class="r"><code>optim_results[&quot;nlm_no_grad&quot;,] &lt;- c(nlm_nograd$value,end - start)</code></pre>
<p>We see if we give it the function without derivatives it takes more iterations.</p>
<div id="simplex-methods" class="section level4">
<h4>Simplex Methods</h4>
<p>The Nelder-Mead is the standard simplex method. It is a directed search comparing values at various points. It has the advantage of not requiring derivatives but it only converges to local minimima, it can converge to non-stationary points and it is slow.</p>
<pre class="r"><code>start = Sys.time()
NM &lt;- optim(c(1, 1), function(x) f(x[1], x[2]), method = &quot;Nelder-Mead&quot;)
end = Sys.time()
NM</code></pre>
<pre><code>## $par
## [1] 0.4691359 0.1267966
## 
## $value
## [1] 1.560592
## 
## $counts
## function gradient 
##       73       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<pre class="r"><code>optim_results[&quot;Nelder-Mead&quot;,] &lt;- c(NM$value,end - start)</code></pre>
<p>We see this takes 73 iterations so we expect this to be quite slow.</p>
</div>
<div id="gradient-type-methods" class="section level4">
<h4>Gradient type methods</h4>
<p>The most popular method here is gradient descent. In order to minimise a differentiable function <span class="math inline">\(f(x)\)</span> we iteratively take: <span class="math display">\[x \leftarrow x - \gamma\nabla f(x) \]</span> where <span class="math inline">\(\gamma\)</span> is a small step value. This is simple but is slow can can zigzag on certain functions. <code>optim</code> uses the conjugate gradient (CG) which is an improved version. The main idea is that each step should be conjugate to the search direction at the previous step to avoid zigzag. If the function is quadratic then CG is guaranteed to reach the minimum in n-steps where function is n-dimensional. It performs better than steepest descent in all cases but not as well as Newton type methods.</p>
<pre class="r"><code>start = Sys.time()
CG &lt;- optim(c(1, 1), function(x) f(x[1], x[2]), method = &quot;CG&quot;)
end = Sys.time()
CG</code></pre>
<pre><code>## $par
## [1] 0.4690988 0.1268843
## 
## $value
## [1] 1.560592
## 
## $counts
## function gradient 
##      168       57 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<pre class="r"><code>optim_results[&quot;CG&quot;,] &lt;- c(CG$value,end - start)</code></pre>
</div>
<div id="newton-type-methods" class="section level4">
<h4>Newton type methods</h4>
<p>In the multi dimensional case Newton’s method becomes:</p>
<p><span class="math display">\[x \leftarrow [H f(x)]^{-1} \nabla f(x) \]</span></p>
<p>where we need the inverse of the Hessian. This can be expensive for multivariate problems as the Hessian can be very large. Quasi-Newton methods aim to replace this computation by a reasonable estimate. The most common quasi-newton method is <code>BFGS</code> which can be implemented by <code>optim</code>. The Hessian is approximated by <span class="math inline">\(B_{k}\)</span> and it’s inverse <span class="math inline">\(B_{k+1}^{-1}\)</span> is computed using <span class="math inline">\(B_{k}^{-1}\)</span>. As <span class="math inline">\(B_{k}\)</span> are dense this can cause memory problems in high dimensions, <code>L-BFGS</code> is the low memory version which stores a few vectors rather than a full approximation.</p>
<pre class="r"><code>start = Sys.time()
BFGS &lt;- optim(c(1, 1), function(x) f(x[1], x[2]), method = &quot;BFGS&quot;)
end = Sys.time()
BFGS</code></pre>
<pre><code>## $par
## [1] -1.0619460 -0.6921332
## 
## $value
## [1] 3.786106
## 
## $counts
## function gradient 
##       20       11 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<pre class="r"><code>optim_results[&quot;BFGS&quot;,] &lt;- c(BFGS$value,end - start)

start = Sys.time()
LBFGS &lt;- optim(c(1, 1), function(x) f(x[1], x[2]), method = &quot;L-BFGS-B&quot;)
end = Sys.time()
LBFGS</code></pre>
<pre><code>## $par
## [1] 0.4691001 0.1268841
## 
## $value
## [1] 1.560592
## 
## $counts
## function gradient 
##        9        9 
## 
## $convergence
## [1] 0
## 
## $message
## [1] &quot;CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH&quot;</code></pre>
<pre class="r"><code>optim_results[&quot;L-BFGS-B&quot;,] &lt;- c(LBFGS$value,end - start)</code></pre>
</div>
<div id="simulated-annealing" class="section level4">
<h4>Simulated annealing</h4>
<p>Simulated annealing claims to always find the global minimum, but it is slow and can often fail to find the global minimum regardless. It only uses the value of the function like the simplex method. In <code>optim</code> the implementation is done with a Metropolis function for acceptance probability.</p>
<pre class="r"><code>start = Sys.time()
SANN &lt;- optim(c(1, 1), function(x) f(x[1], x[2]), method = &quot;SANN&quot;)
end = Sys.time()
SANN</code></pre>
<pre><code>## $par
## [1] 0.4679097 0.1266405
## 
## $value
## [1] 1.560609
## 
## $counts
## function gradient 
##    10000       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<pre class="r"><code>optim_results[&quot;SANN&quot;,] &lt;- c(SANN$value,end - start)</code></pre>
<p>We now look at the results for each of the methods</p>
<pre class="r"><code>optim_results</code></pre>
<pre><code>##                   minimum       times
## nlm_with_grad 0.003000021 0.003000021
## nlm_no_grad   0.002000093 0.002000093
## Nelder-Mead   1.560592049 0.003000975
## CG            1.560592035 0.002999783
## BFGS          3.786106059 0.003000975
## L-BFGS-B      1.560592035 0.002999067
## SANN          1.560608650 0.021004915</code></pre>
<p>We see that for time taken, all the functions perform similarly in this case apart from simulated annealing which takes just over twice as long as the next slowest. Surprisingly BFGS also seems to have encountered some problems as it does not converge to the correct point and ran slightly slower, though the low memory version did converge correctly. Neither of the <code>nlm</code> implementations found the correct minimum although as it is smaller than the expected minimum it seems they have found one outside the area in which we were looking. As we are only looking at minimising one function here these results are far from conclusive as to the performance of the relative functions.</p>
</div>
</div>
<div id="nonlinear-least-squares-problems" class="section level3">
<h3>Nonlinear least squares problems</h3>
<p>With dataset <span class="math inline">\((x_{1}, y_{1}), ...,(x_{m}, y_{m})\)</span> and model <span class="math inline">\(y = f(x, \beta)\)</span> where <span class="math inline">\(\beta\)</span> are the parameters of the model <span class="math inline">\(f\)</span> we define residuals <span class="math inline">\(r_{i} (\beta) = y_{i} - f(x_{i}, \beta)\)</span>. In a nonlinear least squares problem we aim to find <span class="math inline">\(\beta\)</span> such that the sum of squared residuals is minimised. There a subclass of optimisation algorithms designed for nonlinear functions.</p>
<p>The Gauss-Newton algorithm is related to Newton’s method. The difference is that Gauss-Newton approximates the Hessian matrix of the objective function using Jacobian matrix <span class="math inline">\(J_{r}\)</span> of the residue function. It approximates the Hessian with <span class="math inline">\(J_{r}^{t}J_{r}\)</span> and then iteratively does: <span class="math display">\[\beta \leftarrow \beta - (J_{r}^{t}J_{r})^{-1}J_{r}^{t}r(\beta)\]</span> This works well in practice but is not guaranteed to converge. The Levenberg-Marquadt algorithm addresses this using damping, instead pretending the Hessian is <span class="math inline">\(J_{r}^{t}J_{r} + \lambda I\)</span>. This is guaranteed to be invertible for some <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div id="stochastic-gradient-descent" class="section level3">
<h3>Stochastic gradient descent</h3>
<p>So far the algorithms have all computed gradient vectors using all of the dataset. In big data applications, the dataset can be too large to hold in memory. This motivates Stochastic Gradient Descent (SGD). This takes the gradient direction using only a few data samples. It is popular for training neural networks where full back propagation is computationally expensive. The simplest form is: <span class="math display">\[\beta \leftarrow \beta - \gamma r_{i} \nabla r_{i}(\beta)\]</span> where <span class="math inline">\(i\)</span> cycles through all data points. It can also be done by combining data samples into ‘minibatches’ and updating <span class="math inline">\(\beta\)</span> with the samples in each minibatch.</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
